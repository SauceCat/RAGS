# ARES: Answer Relevance

- **Dimension:** Generated Answer <-> Retrieved Context, Question
- **Reference:** [ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2311.09476)
- **Type:** LLM classifier

Answer Relevance: Is the answer generated relevant given the query and retrieved passage?

Fine-tune `DeBERTa-v3-Large` with a binary classifier head for Answer Relevance. Use a human preference validation set to assess model improvement after each epoch.

## Training Data

Positive Samples: 
- The answers are generated for each passage-query pair using `FLAN-T5 XXL`, providing natural positive samples where the passage-question-answer relationship is known.

Negative samples:
- Weak negative: Randomly sampled synthetically-generated answers from other passages.
- Strong negative: Contradictory answers generated by `FLAN-T5 XXL` using a modified few-shot prompt:

For generating synthetic answers, the following prompt is used for `FLAN-T5 XXL`:
```
Example #1
Query: <few-shot example here>
Document: <few-shot example here>
Answer: <few-shot example here>
Example #2
Query: <few-shot example here>
Document: <few-shot example here>
Answer: <few-shot example here>
Example #3
Query: <few-shot example here>
Document: <few-shot example here>
Answer: <few-shot example here>
Example #4
Query: <synthetic query here>
Document: <in-domain passage here>
Answer:
```

The prompting structure remains consistent for both correct and incorrect answer generation, with the few-shot examples adjusted to produce the desired output type.

```
Example N
Question: [few-shot example with incorrect or contradictory answer]
Document: [few-shot example]
Answer: [few-shot example]
```

## Limitations
Same as [Answer Faithfulness](ARES_answer-faithfulness.md).