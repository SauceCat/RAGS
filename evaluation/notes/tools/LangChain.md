# LangChain
**Reference:**
- [Evaluating RAG Architectures on Benchmark Tasks](https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/comparing_techniques.html)
- [langchain_benchmarks/rag/evaluators.py](https://github.com/langchain-ai/langchain-benchmarks/blob/main/langchain_benchmarks/rag/evaluators.py)

The `langchain_benchmarks` repository from LangChain offers support for several basic RAG evaluation metrics. Key features include:

- **Simple RAG Evaluation Metrics**: Implements fundamental metrics for assessing RAG system performance through semantic similarity and LLM-as-a-judge.
- **LangSmith Integration:** Offers visualization and tracking capabilities through LangSmith, enabling ongoing monitoring and analysis of RAG system metrics.
- **Accessibility:** These tools make RAG evaluation more accessible to developers using the LangChain framework.

This implementation allows for straightforward assessment and monitoring of RAG systems, facilitating iterative improvements in retrieval and generation processes.

<img src="../images/tools/LangChain.png" width=1000>